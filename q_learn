import gymnasium as gym
import numpy as np
import random

# step() returns {Position, Reward, Termination, Truncation}


EPSILON = 0.99
GAMES_LIMIT = 20000
LR = 0.01
GAMMA = 0.9



def init(saved: bool):
    
    if saved:
        q_table = np.load("q_table.npy")
    else:
        q_table = np.zeros(shape=(16, 4))
    
    return q_table

def print_info(n_games, reward, n_steps):
    print(f"Game: {n_games} | Reward: {reward} | Steps: {n_steps}")
    
def test_agent(env:gym.Env, q_table, episodes=100):
    total_rewards = 0

    for episode in range(episodes):
        state, _ = env.reset()
        done = False

        while not done:
            # Política completamente greedy (epsilon = 0)
            action = np.argmax(q_table[state])
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            total_rewards += reward

    avg_reward = total_rewards / episodes
    print(f"Recompensa media en {episodes} episodios (ε=0): {avg_reward:.2f}")
    return avg_reward

def train_agent(env:gym.Env, q_table):

    n_games = 1
    
    #Por cada serie/partida
    while n_games <= GAMES_LIMIT:
        
        termination, truncation = False, False
        done = termination + truncation
        
        state, _ = env.reset() 
        n_steps = 1
        
        while not done:
            
            # Update the epsilon value
            epsilon = max(0.1, EPSILON * (0.995 ** n_games))
            
            # Use epsilon-greedy
            random_float = random.random()
            if random_float < epsilon:
                # Explore
                action = env.action_space.sample()
            else:
                # Act greedy (explotation)
                action = np.argmax(q_table[state])
                
            # We perform the neww action
            new_state, reward, termination, truncation, _ = env.step(action)
            
            #Adjust (optional)
            if termination == True and reward == 0:
                reward = -1
            
            # We calculate the new q value with Q(s,a) = Q(s,a) + α * [r + γ * max(Q(s',a')) - Q(s,a)]
            new_q_value = q_table[state][action] + LR * (reward + GAMMA * q_table[new_state][np.argmax(q_table[new_state])] - q_table[state][action])
            
            # Update the value
            q_table[state][action] = new_q_value
            
            # Update the variables
            state = new_state
            done = termination + truncation
            n_steps += 1
            
        if n_games % 20 == 0:    
            print_info(n_games, reward, n_steps)
        
        # Increment the game´s counter
        n_games += 1
            
    np.save("q_table.npy", q_table)
    return q_table

    
if __name__ == "__main__":
    
    # Inicializamos la q_table con 0s
    q_table = init(False)
    
    #Creamos el entorno
    env = gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True)
    q_table = train_agent(env, q_table)
    test_agent(env, q_table)
    
    env = gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True, render_mode="human")
    test_agent(env, q_table, episodes=10)
    
    
    
    